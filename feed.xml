<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://himal2007.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://himal2007.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-01T02:47:58+00:00</updated><id>https://himal2007.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Checking your GPU via WSL2</title><link href="https://himal2007.github.io/blog/2024/GPU-WSL2/" rel="alternate" type="text/html" title="Checking your GPU via WSL2"/><published>2024-12-17T15:59:00+00:00</published><updated>2024-12-17T15:59:00+00:00</updated><id>https://himal2007.github.io/blog/2024/GPU-WSL2</id><content type="html" xml:base="https://himal2007.github.io/blog/2024/GPU-WSL2/"><![CDATA[<h2 id="checking-your-nvidia-gpu-via-wsl2">Checking your NVIDIA GPU via WSL2</h2> <p>Like many computer science enthusiasts, I had a dream to build a powerful desktop that would make even the most ardent gamers-or in my case, bioinformaticians-pause in admiration. So, I went ahead and purchased a powerhouse machine, armed with a high-performance GPU packed with CUDA cores, perfect for deep learning and computational analysis. I also aptly named it ‚ÄúThe Beast‚Äù. Yet, my precious Beast has been sitting idle (for nearly two years after purchase) when it comes to actual data analysis. Sound familiar? If you‚Äôre a Windows user like me, sitting on untapped GPU potential, this is the guide for you on how to inspect your GPU via windows subsystem for linux 2 (WSL2). At least we got to start somewhere with harnessing the full potential of your NVIDIA GPU (most have NVIDIA GPUs, some have AMD GPUs) within the Windows ecosystem. Let me walk you through on inspecting your GPU via WSL2. Please note that this guide is for NVIDIA GPUs. Similar alternatives exist for AMD GPUs.</p> <h3 id="step-1-identifying-your-gpu">Step 1: Identifying Your GPU</h3> <p>To find out how many GPUs you have in WSL2, first, open your WSL2 terminal and check details using the <code class="language-plaintext highlighter-rouge">lspci</code> command. you might try using the <code class="language-plaintext highlighter-rouge">lspci</code> command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>lspci | <span class="nb">grep</span> <span class="nt">-i</span> vga
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">lspci</code> lists all the Peripheral Component Interconnect (PCI) devices in your system, which includes network cards, sound cards, and GPUs. The <code class="language-plaintext highlighter-rouge">grep -i vga</code> part filters the output to show only VGA-compatible devices, which typically include your GPU. However, this command doesn‚Äôt always display your GPU in WSL2 (in my case as well). In that case, using the <code class="language-plaintext highlighter-rouge">nvidia-smi</code> command (<a href="https://unix.stackexchange.com/questions/370510/nvidia-smi-equivalent-for-amd-apu">alternative for AMD GPUs is <code class="language-plaintext highlighter-rouge">rocm-smi</code></a>) is a more reliable way to check your GPU.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvidia-smi
</code></pre></div></div> <p>This command provides detailed information about your GPU, including its name, driver version, and memory usage. For example, a typical output might look like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.02              Driver Version: 560.94         CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2060        On  |   XXXXXX:XXX.XX:XX  On |                  N/A |
|  0%   45C    P8             13W /  170W |     817MiB /   6144MiB |      6%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
</code></pre></div></div> <h3 id="step-2-understanding-the-output">Step 2: Understanding the Output</h3> <p>The <code class="language-plaintext highlighter-rouge">nvidia-smi</code> command reveals a wealth of information. The key sections to focus on include:</p> <ul> <li>The first row displays the <strong>NVIDIA-SMI version</strong>, <strong>Driver Version</strong>, and <strong>CUDA Version</strong>.</li> <li><strong>NVIDIA-SMI Version:</strong> Indicates the version of the NVIDIA System Management Interface (SMI) tool you‚Äôre using.</li> <li><strong>Driver Version and CUDA Version:</strong> Ensures you have the right software for GPU-accelerated tasks.</li> <li>In the lower two rows, the upper row is the header, and the lower row is the actual data. This has information about the actual GPU Name, memory usage, GPU utilisation, etc.</li> <li><strong>GPU Details:</strong> Shows the model (NVIDIA GeForce RTX 2060) and important metrics like temperature and power usage</li> <li><strong>Memory Usage:</strong> Indicates how much GPU memory is used and available.</li> <li><strong>GPU Utilisation:</strong> Provides insights into how much of your GPU‚Äôs capacity is being used.</li> </ul> <blockquote> <p>üìù You can use other versions of the command - <code class="language-plaintext highlighter-rouge">nvidia-smi -L</code> to list the GPUs in your system and <code class="language-plaintext highlighter-rouge">nvidia-smi -q</code> to get detailed information about your GPU.</p> </blockquote> <h3 id="step-3-counting-your-gpu-cores">Step 3: Counting Your GPU Cores</h3> <p>If you‚Äôre curious about the number of CUDA cores in your GPU, while <code class="language-plaintext highlighter-rouge">nvidia-smi</code> doesn‚Äôt directly provide this information, you can look up your GPU model‚Äôs specifications. For instance, the <a href="https://www.google.com.au/search?q=NVIDIA+GeForce+RTX+2060+number+of+cores&amp;newwindow=1&amp;sca_esv=d8dafcc4ab3fc50a&amp;sxsrf=ADLYWII8hMarNEZYed2PKUl4mFf4xsex_w%3A1735072723984&amp;ei=0xtrZ9rfO7DLseMPma3H-QQ&amp;ved=0ahUKEwialf3KocGKAxWwZWwGHZnWMU8Q4dUDCBA&amp;uact=5&amp;oq=NVIDIA+GeForce+RTX+2060+number+of+cores&amp;gs_lp=Egxnd3Mtd2l6LXNlcnAiJ05WSURJQSBHZUZvcmNlIFJUWCAyMDYwIG51bWJlciBvZiBjb3JlczIGEAAYFhgeMgYQABgWGB4yBhAAGBYYHjILEAAYgAQYhgMYigUyCxAAGIAEGIYDGIoFMgsQABiABBiGAxiKBTIIEAAYogQYiQUyBRAAGO8FMgUQABjvBTIFEAAY7wVIwxtQVVi0GnADeAGQAQCYAe0FoAHYHaoBDTAuNS4yLjAuMi4wLjK4AQPIAQD4AQGYAg6gAqIfwgIFECEYoAHCAggQABgWGAoYHsICBxAhGKABGAqYAwCSBwszLjMuNC4wLjIuMqAHgEg&amp;sclient=gws-wiz-serp">NVIDIA GeForce RTX 2060</a> typically has <strong>1920 CUDA cores</strong>, <strong>240 Tensor cores</strong> and <strong>30 RT cores</strong>. Note that a GPU can have different types of cores, each serving a specific purpose:</p> <ul> <li><strong>CUDA Cores</strong>: Versatile for general computation and rendering.</li> <li><strong>Tensor Cores</strong>: Specialized for deep learning tasks, speeding up matrix-heavy operations.</li> <li><strong>RT Cores</strong>: Focused on real-time ray tracing, revolutionizing visual realism in graphics.</li> </ul> <p>You might want to find this information programmatically, for which you might need to rely on Python‚Äôs numba module (open source compiler for translating Python and NumPy code into a fast machine code). Here‚Äôs a Python script (thanks to <a href="https://stackoverflow.com/questions/63823395/how-can-i-get-the-number-of-cuda-cores-in-my-gpu-using-python-and-numba">StackOverflow post</a>) that can help you <strong>identify the number of CUDA cores</strong> in your GPU:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">numba</span> <span class="kn">import</span> <span class="n">cuda</span>

<span class="c1"># Dictionary mapping compute capabilities to cores per SM
</span><span class="n">cc_cores_per_SM_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="mi">32</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span> <span class="mi">48</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="mi">192</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span> <span class="mi">192</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">):</span> <span class="mi">192</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="mi">128</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span> <span class="mi">128</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="mi">64</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span> <span class="mi">128</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="mi">64</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span> <span class="mi">64</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="mi">64</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">):</span> <span class="mi">128</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">):</span> <span class="mi">128</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="mi">128</span>
<span class="p">}</span>

<span class="c1"># Get the current device
</span><span class="n">device</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">get_current_device</span><span class="p">()</span>

<span class="c1"># Retrieve the number of SMs
</span><span class="n">num_sms</span> <span class="o">=</span> <span class="n">device</span><span class="p">.</span><span class="n">MULTIPROCESSOR_COUNT</span>

<span class="c1"># Retrieve the compute capability
</span><span class="n">compute_capability</span> <span class="o">=</span> <span class="n">device</span><span class="p">.</span><span class="n">compute_capability</span>

<span class="c1"># Get the number of cores per SM based on compute capability
</span><span class="n">cores_per_sm</span> <span class="o">=</span> <span class="n">cc_cores_per_SM_dict</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">compute_capability</span><span class="p">,</span> <span class="sh">"</span><span class="s">Unknown</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Calculate total cores
</span><span class="k">if</span> <span class="n">cores_per_sm</span> <span class="o">!=</span> <span class="sh">"</span><span class="s">Unknown</span><span class="sh">"</span><span class="p">:</span>
    <span class="n">total_cores</span> <span class="o">=</span> <span class="n">cores_per_sm</span> <span class="o">*</span> <span class="n">num_sms</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">GPU Compute Capability: </span><span class="si">{</span><span class="n">compute_capability</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Number of SMs: </span><span class="si">{</span><span class="n">num_sms</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Cores per SM: </span><span class="si">{</span><span class="n">cores_per_sm</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Total CUDA Cores: </span><span class="si">{</span><span class="n">total_cores</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Compute capability </span><span class="si">{</span><span class="n">compute_capability</span><span class="si">}</span><span class="s"> is not recognized.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>SM</strong> here in the code refers to Streaming Multiprocessor (SM) which is a fundamental unit of parallel processing in GPUs (analogous to cores in CPUs). Each SM contains multiple CUDA cores, along with other resources like shared memory and instruction schedulers. SMs are responsible for executing instructions and performing computations in parallel, making them crucial for the GPU‚Äôs performance.</p> <p>This script will help you identify the number of CUDA cores in your GPU. This is the output I got for my NVIDIA GeForce RTX 2060:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>GPU Compute Capability: (7, 5)
Number of SMs: 30
Cores per SM: 64
Total CUDA Cores: 1920
</code></pre></div></div> <p>With these steps, you can at least inspect your GPU and understand its specifications. This knowledge will be crucial as you dive into GPU-accelerated data analysis, machine learning, and other computational tasks. Happy computing!</p> <h3 id="definitions">Definitions</h3> <p>There are mutliple types of cores in a GPU. It‚Äôs good to be aware of the different types of cores in a GPU.</p> <h3 id="additional-resources">Additional Resources</h3> <ul> <li><a href="https://learn.microsoft.com/en-us/windows/ai/directml/gpu-accelerated-training">GPU-accelerated ML Training with WSL</a></li> <li><a href="https://numba.readthedocs.io/en/stable/index.html"><code class="language-plaintext highlighter-rouge">numba</code> documentation</a></li> <li><a href="https://numba.readthedocs.io/en/0.52.0/roc/index.html">Numba for AMD ROC GPUs</a></li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="external-services"/><category term="GPU"/><category term="WSL2"/><category term="CUDA"/><category term="NVIDIA"/><summary type="html"><![CDATA[A guide to help you identify your GPU and its specifications via Windows Subsystem for Linux 2 (WSL2).]]></summary></entry><entry><title type="html">Creating Flowcharts with Mermaid</title><link href="https://himal2007.github.io/blog/2024/mermaid/" rel="alternate" type="text/html" title="Creating Flowcharts with Mermaid"/><published>2024-06-04T17:05:00+00:00</published><updated>2024-06-04T17:05:00+00:00</updated><id>https://himal2007.github.io/blog/2024/mermaid</id><content type="html" xml:base="https://himal2007.github.io/blog/2024/mermaid/"><![CDATA[<h1 id="creating-flowcharts-with-mermaid">Creating Flowcharts with Mermaid</h1> <p>Flowcharts are an integral part of my work. Whether it‚Äôs mapping out processes, planning projects, or visualizing ideas, a good flowchart can make all the difference. Over time, I‚Äôve experimented with various tools to create these diagrams. Here, I want to share my experience with Mermaid, a powerful tool for creating flowcharts and other diagrams using simple code.</p> <h2 id="the-hunt-for-the-perfect-flowchart-tool">The Hunt for the Perfect Flowchart Tool</h2> <p>I‚Äôve tried several tools in my quest for the perfect flowchart creator. Here are some honorable mentions:</p> <ul> <li><strong>draw.io</strong>: A versatile and user-friendly diagramming tool.</li> <li><strong>Excalidraw</strong>: Great for hand-drawn style diagrams and recently added support for Mermaid.</li> <li><strong>Lucidchart</strong>: A robust, professional-grade diagramming application.</li> </ul> <p>These tools are fantastic, but I needed something that could be seamlessly embedded within markdown documents, which led me to discover Mermaid.</p> <h2 id="what-is-mermaid">What is Mermaid?</h2> <p>Mermaid is a tool that allows you to create diagrams and visualizations using a simple and easy-to-read code syntax. It‚Äôs particularly useful for embedding within markdown documents, making it perfect for documentation and collaborative projects.</p> <h3 id="why-mermaid">Why Mermaid?</h3> <ol> <li><strong>Markdown Integration</strong>: Mermaid lets you embed flowcharts directly within your markdown files. This keeps everything in one place and makes your documents more dynamic and interactive.</li> <li><strong>Code-based Diagrams</strong>: With Mermaid, you write diagrams as code. This approach is not only efficient but also allows for version control and easy updates.</li> <li><strong>Versatility</strong>: Beyond flowcharts, Mermaid supports Gantt charts, sequence diagrams, class diagrams, and more.</li> </ol> <h3 id="creating-a-flowchart-with-mermaid">Creating a Flowchart with Mermaid</h3> <p>Here‚Äôs an example of how you can create a simple flowchart using Mermaid:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">mermaid
</span><span class="sb">graph TD
    A[Start] --&gt; B{Is it working?}
    B --&gt;|Yes| C[Continue]
    B --&gt;|No| D[Fix it]
    D --&gt; B</span>
<span class="p">```</span>
</code></pre></div></div> <p>This code snippet generates a flowchart below with decision points and actions. Make sure to add to add <code class="language-plaintext highlighter-rouge">mermaid</code> tag in the code block to enable Mermaid rendering. Also, you need to enable Mermaid rendering in the YAML front matter of your markdown file with <code class="language-plaintext highlighter-rouge">mermaid: enabled: true</code>.</p> <pre><code class="language-mermaid">graph TD
    A[Start] --&gt; B{Is it working?}
    B --&gt;|Yes| C[Continue]
    B --&gt;|No| D[Fix it]
    D --&gt; B
</code></pre> <h2 id="exporting-charts">Exporting Charts</h2> <p>One of my initial concerns was how to export these charts in high-quality formats like PDF or PNG. Thankfully, I found the <strong>Mermaid Live Editor</strong>. This online tool allows you to create, preview, and export your Mermaid diagrams easily. It‚Äôs very similar to Leaflet and operates on a freemium model.</p> <h2 id="interactive-features">Interactive Features</h2> <p>When rendered within a markdown document, Mermaid charts offer neat interactive features:</p> <ul> <li><strong>Zoom</strong>: Easily zoom in and out to focus on different parts of your chart.</li> <li><strong>Rotate</strong>: Rotate your diagrams to get a better view or fit them into your layout.</li> </ul> <h2 id="extra-capabilities-with-mermaid">Extra Capabilities with Mermaid</h2> <p>Mermaid is continuously evolving, and there are several additional features worth mentioning:</p> <ul> <li><strong>Integration with Excalidraw</strong>: Excalidraw now supports Mermaid, allowing you to combine hand-drawn elements with code-based diagrams.</li> <li><strong>AI Features</strong>: Tools like GitHub Copilot can help you generate flowcharts from text descriptions. For example, using <a href="https://github.com/marketplace/actions/export-mermaidjs-erdiagrams-from-database">this GitHub Action</a>, you can convert database schemas to ER diagrams with ease.</li> <li><strong>Image Conversion</strong>: You can even convert images into flowcharts using advanced AI capabilities.</li> </ul> <p>In conclusion, Mermaid is a fantastic tool for anyone who needs to create and embed flowcharts and other diagrams within markdown documents. It‚Äôs easy to use, highly versatile, and offers excellent integration with other tools and platforms. If you‚Äôre in the market for a flowchart tool, give Mermaid a try ‚Äì it might just be the perfect solution for your needs!</p>]]></content><author><name></name></author><category term="tools"/><category term="flowcharts"/><category term="mermaid"/><summary type="html"><![CDATA[My experience with mermaid for creating flowcharts and diagrams using simple code.]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://himal2007.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://himal2007.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://himal2007.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We‚Äôre sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://himal2007.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://himal2007.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://himal2007.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>